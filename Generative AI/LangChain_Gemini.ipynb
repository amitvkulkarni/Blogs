{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Langchain | Shaping the Future of AI Development through Advanced Frameworks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"AI2.png\" alt=\"drawing\" style=\"width:800px;\"/> -->\n",
    "<!-- <div style=\"text-align:center\"><img src=\"AI2.png\"></div> -->\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"65%\" src=\"AI2.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Introduction\n",
    "* What is Langchain?\n",
    "* Setting up the Gemini API\n",
    "* Getting started\n",
    "* Features of Langchain\n",
    "    - Prompts & templates\n",
    "    - Power of chains\n",
    "* Features of Langchain\n",
    "    - Schema & Message\n",
    "    - Message prompt template\n",
    "    - Memory\n",
    "    - Agents\n",
    "* Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Introduction**\n",
    "Generative AI is a significant advancement in artificial intelligence, revolutionizing content creation and understanding. The LLM models that power the Genertive AI can understand context, generate coherent responses, and perform tasks based on user prompts. However, the true potential of Generative AI is realized when paired with frameworks like Langchain. Langchain acts as a catalyst, enabling developers to harness the full capabilities of LLMs like Google’s Gemini AI and OpenAI’s models. This integration provides streamlined access to advanced language processing capabilities, unlocking new possibilities for intelligent and interactive applications that can understand, respond to, and anticipate user needs. This blog explores the transformative power of AI advancements with Google Gemini and their impact on the AI landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kulkarna4029\\OneDrive - ARCADIS\\Desktop\\My Apps\\Data-Apps\\Generative AI\\OCR App With Gemini and Dash\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **What is Google’s Gemini API?**\n",
    "Google’s Gemini AI is a significant advancement in artificial intelligence, particularly in natural language processing (NLP). It provides developers with advanced NLP models trained on vast text data, enabling tasks like text generation, sentiment analysis, and language translation. When paired with Langchain, an innovative AI framework, developers can unlock the full potential of Gemini AI to build sophisticated conversational AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Setting up the API Key**\n",
    "Google offers users the ability to create an API key through its AI studio. This key can be securely stored and easily incorporated into code, similar to other AI tools. To do this, we use the .env file to store the API key, then load it into the code. Below is an example demonstrating this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Available models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the Gemini API to retrieve a list of South-East Asian countries. Using our API key, we’ll connect to the “gemini-pro” model and pose a question to the model. The model will then generate a response containing the requested information. Let’s proceed with the code to fetch the desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_response(input):\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    response = model.generate_content(input)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " * Brunei\n",
      "* Cambodia\n",
      "* East Timor\n",
      "* Indonesia\n",
      "* Laos\n",
      "* Malaysia\n",
      "* Myanmar\n",
      "* Philippines\n",
      "* Singapore\n",
      "* Thailand\n",
      "* Vietnam\n"
     ]
    }
   ],
   "source": [
    "question = \"List the south east asian countries\"\n",
    "response = get_gemini_response(question)\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was straightforward. In the next section, we’ll dive into how we can enhance AI responses by providing specific inputs tailored to different contexts. For example, imagine wanting the AI to adopt the persona of a comic character and generate humorous responses. Alternatively, you might need the AI to act as a technical assistant, providing insightful answers to your queries. We can even take it a step further by allowing the AI to make decisions on how to gather information — whether it’s from documents, search engines, or other sources. By incorporating these additional layers of input, we can guide the AI to deliver more relevant and engaging responses. Join us as we explore how to implement these enhancements using Langchain, making interactions with AI even more dynamic and effective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **What is Langchain?**\n",
    "Langchain is a tool that enables developers to access Gemini AI’s advanced features, allowing them to create text, understand sentiment, and more through a user-friendly interface. It is designed to be flexible, allowing developers to customize it to fit their needs. Langchain uses big language models to create interactive apps that interact with users naturally. It also allows developers to add prompts and receive responses from these models, making it easier to create smart, interactive apps. Langchain also has modules compatible with popular language models like Hugging Face, Open AI, and Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Prompts**\n",
    "Langchain’s prompt system provides structured input for AI models like Gemini, allowing developers to formulate specific questions or commands. Customizing prompts allows users to steer AI responses towards desired outcomes, such as generating relevant information or completing tasks. This simplifies the interaction process, enabling developers to optimize AI interactions for various use cases, enhance user experiences, and drive innovation in AI-powered applications.\n",
    "\n",
    "Let’s try the same query from the previous section and use Langchain to get the response from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Brunei\n",
      "* Cambodia\n",
      "* East Timor\n",
      "* Indonesia\n",
      "* Laos\n",
      "* Malaysia\n",
      "* Myanmar (Burma)\n",
      "* Philippines\n",
      "* Singapore\n",
      "* Thailand\n",
      "* Vietnam\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Lets the same using Langchain interface\n",
    "###########################################################################\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "result = gemini_llm.invoke(\"List the south east asian countries\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='* Brunei\\n* Cambodia\\n* East Timor\\n* Indonesia\\n* Laos\\n' response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}\n",
      "content='* Malaysia\\n* Myanmar\\n* Philippines\\n* Singapore\\n* Thailand\\n* Vietnam' response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}\n"
     ]
    }
   ],
   "source": [
    "for chunk in gemini_llm.stream(\"List the south east asian countries\"):\n",
    "    print(chunk, end=\"\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Prompt templates**\n",
    "Langchain’s prompt templates are extensions of prompts, offering predefined structures for interacting with AI models. They provide a standardized format for inputting queries or commands, streamlining the interaction process. Developers can create consistent prompts tailored to their use cases, ensuring clarity and coherence in AI interactions. These templates enhance usability and optimize performance in various applications.\n",
    "\n",
    "Let’s understand the prompt templates with an example. We wish to get a famous fruit from a given country and in this case, the country will be the input and the fruit will be the output.\n",
    "\n",
    "we import the PromptTemplate and define the input_variables and the template.\n",
    "We will format the query by feeding the country name and the output will be a proper sentence as seen in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which is the famous fruit from India'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"country\"], template=\"Which is the famous fruit from {country}\"\n",
    ")\n",
    "\n",
    "prompt_template.format(country=\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good but what if we wish to give multiple inputs or have multiple templates? How can that be achieved? We will explore that in the next section with chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Power of chains**\n",
    "Langchain’s chains are essential for facilitating interactions between users and AI models. They come in sequential, conditional, and conversational chains, each serving a specific purpose. Sequential chains ensure a linear progression of tasks, conditional chains introduce branching logic, and conversational chains enable dynamic exchanges. Chains structure AI interactions, enhance usability, and enable developers to create sophisticated AI applications with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Simple sequential chains**\n",
    "We will extend the previous example of a famous fruit from a given country. We will create multiple templates and chain them together using LLMChain.\n",
    "\n",
    "Country_template: Captures the input from the user i.e. country.\n",
    "Country_chain: Create a chain with the template 1 and the LLM model instance\n",
    "Famous_template: The output from Template 1 will be the input to Template 2. Also, Template 2 will output the health benefits from the fruit.\n",
    "Famous_chain: Create a chain with the template 2 and the LLM model instance\n",
    "SimpleSequentialChain: Use LLMChain to bring Country_chain and Famous_chain together. We will run the chain to get the output and in this case for input — India, the famous fruit came out to be Mango and described health benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kulkarna4029\\OneDrive - ARCADIS\\Desktop\\My Apps\\Data-Apps\\Generative AI\\OCR App With Gemini and Dash\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mango\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**Nutritional Value:**\\n\\n* Rich in vitamins A, C, and E\\n* Good source of fiber, potassium, and magnesium\\n* Contains antioxidants such as beta-carotene and quercetin\\n\\n**Health Benefits:**\\n\\n**1. Eye Health:**\\n* Vitamin A in mangoes supports eye health and prevents night blindness.\\n* Beta-carotene acts as an antioxidant, protecting the eyes from damage caused by free radicals.\\n\\n**2. Immune System Boost:**\\n* Vitamin C is essential for a strong immune system.\\n* Antioxidants in mangoes help fight infections and reduce inflammation.\\n\\n**3. Digestion:**\\n* Fiber in mangoes aids digestion and prevents constipation.\\n* Enzymes in mangoes help break down proteins and improve digestion.\\n\\n**4. Heart Health:**\\n* Potassium in mangoes helps regulate blood pressure and reduce the risk of heart disease.\\n* Fiber helps lower cholesterol levels.\\n\\n**5. Skin Health:**\\n* Vitamin C promotes collagen production, which is essential for healthy skin.\\n* Antioxidants in mangoes protect the skin from damage caused by UV rays.\\n\\n**6. Cancer Prevention:**\\n* Antioxidants in mangoes, such as quercetin and beta-carotene, have been linked to a reduced risk of certain types of cancer, including lung, prostate, and breast cancer.\\n\\n**7. Brain Health:**\\n* Vitamin E in mangoes supports brain function and protects against cognitive decline.\\n* Antioxidants in mangoes help reduce inflammation in the brain.\\n\\n**8. Bone Health:**\\n* Magnesium in mangoes is essential for bone health and prevents osteoporosis.\\n* Vitamin K in mangoes helps improve calcium absorption.\\n\\n**9. Weight Management:**\\n* Fiber in mangoes promotes satiety and helps control appetite.\\n* Mangoes are a low-calorie fruit that can be incorporated into a healthy diet.\\n\\n**10. Other Benefits:**\\n* May reduce the risk of asthma\\n* May improve sleep quality\\n* May have anti-aging properties'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################################################\n",
    "# Simple sequential chains\n",
    "############################################################################\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=gemini_llm, prompt=prompt_template)\n",
    "print(chain.run(\"India\"))\n",
    "\n",
    "\n",
    "country_template = PromptTemplate(\n",
    "    input_variables=[\"country\"], template=\"Which is the famous fruit from {country}\"\n",
    ")\n",
    "\n",
    "country_chain = LLMChain(llm=gemini_llm, prompt=country_template)\n",
    "\n",
    "famous_template = PromptTemplate(\n",
    "    input_variables=[\"Fruit\"],\n",
    "    template=\"What are the health benefits from {Fruit}\",\n",
    ")\n",
    "\n",
    "famous_chain = LLMChain(llm=gemini_llm, prompt=famous_template)\n",
    "\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[country_chain, famous_chain])\n",
    "chain.run(\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully chained the templates, processed a sequence of queries, and got the output. Can we make the code and the output much cleaner or easier to read? Let’s try that in the next section using Sequence chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Sequential chains**\n",
    "This is very similar to the previous section but we explicitly specify the inputs and the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kulkarna4029\\OneDrive - ARCADIS\\Desktop\\My Apps\\Data-Apps\\Generative AI\\OCR App With Gemini and Dash\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'country': 'India',\n",
       " 'fruit': 'Mango',\n",
       " 'benefits': \"**Nutritional Value:**\\n\\n* Rich in vitamins A, C, and E\\n* Good source of dietary fiber\\n* Contains antioxidants like beta-carotene and quercetin\\n\\n**Health Benefits:**\\n\\n**1. Improved Digestion:**\\n* Dietary fiber promotes regular bowel movements and prevents constipation.\\n* Enzymes in mangoes help break down proteins and aid digestion.\\n\\n**2. Enhanced Vision:**\\n* Vitamin A is essential for maintaining healthy vision.\\n* Beta-carotene, an antioxidant found in mangoes, is converted to vitamin A in the body.\\n\\n**3. Boosted Immunity:**\\n* Vitamin C is a powerful antioxidant that supports the immune system.\\n* Mangoes also contain quercetin, which has anti-inflammatory and antiviral properties.\\n\\n**4. Reduced Risk of Chronic Diseases:**\\n* Antioxidants in mangoes, such as beta-carotene and quercetin, protect cells from damage caused by free radicals.\\n* This may reduce the risk of chronic diseases like heart disease, cancer, and Alzheimer's.\\n\\n**5. Improved Skin Health:**\\n* Vitamin C is essential for collagen production, which gives skin its elasticity and firmness.\\n* Antioxidants in mangoes help protect the skin from sun damage and premature aging.\\n\\n**6. Reduced Inflammation:**\\n* Quercetin has anti-inflammatory properties that may help reduce inflammation throughout the body.\\n* This can benefit conditions like arthritis, asthma, and inflammatory bowel disease.\\n\\n**7. Lowered Cholesterol Levels:**\\n* Dietary fiber in mangoes helps reduce cholesterol absorption in the digestive tract.\\n* This can help lower LDL (bad) cholesterol levels and improve heart health.\\n\\n**8. Improved Brain Function:**\\n* Antioxidants in mangoes may protect brain cells from damage and improve cognitive function.\\n* Vitamin C is also essential for neurotransmitter production, which supports brain communication.\\n\\n**9. Reduced Risk of Diabetes:**\\n* Dietary fiber in mangoes helps slow down the absorption of sugar into the bloodstream.\\n* This can help prevent spikes in blood sugar levels and reduce the risk of type 2 diabetes.\\n\\n**10. Improved Sleep Quality:**\\n* Mangoes contain tryptophan, an amino acid that promotes relaxation and sleep.\\n* Eating mangoes before bed may help improve sleep quality and duration.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "############################################################################\n",
    "# Sequential chains\n",
    "############################################################################\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "\n",
    "country_template = PromptTemplate(\n",
    "    input_variables=[\"country\"], template=\"Which is the famous fruit from {country}\"\n",
    ")\n",
    "country_chain = LLMChain(llm=gemini_llm, prompt=country_template, output_key=\"fruit\")\n",
    "# country_chain.run(\"India\")\n",
    "\n",
    "benefit_template = PromptTemplate(\n",
    "    input_variables=[\"fruit\"],\n",
    "    template=\"What are the health benefits from {fruit}\",\n",
    ")\n",
    "benefit_chain = LLMChain(llm=gemini_llm, prompt=benefit_template, output_key=\"benefits\")\n",
    "# benefit_chain.run('Mango')\n",
    "\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains=[country_chain, benefit_chain],\n",
    "    input_variables=[\"country\"],\n",
    "    output_variables=[\"fruit\", \"benefits\"],\n",
    ")\n",
    "\n",
    "chain({\"country\": \"India\"})\n",
    "# chain('France')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was much better, we specified the input as country and output as fruit and benefits. The output also shows the sequence of execution that is much cleaner than the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kulkarna4029\\OneDrive - ARCADIS\\Desktop\\My Apps\\Data-Apps\\Generative AI\\OCR App With Gemini and Dash\\genai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"**Factors to Consider:**\\n\\n* **Career Goals:**\\n    * R is widely used in data science, statistics, and bioinformatics.\\n    * Python is versatile and applicable in various fields, including data science, machine learning, web development, and automation.\\n\\n* **Industry Trends:**\\n    * Both R and Python are popular in data science, but Python has gained more traction in recent years.\\n    * Python is also widely used in other industries, making it a more versatile choice.\\n\\n* **Learning Curve:**\\n    * R has a steeper learning curve for beginners, especially if you don't have a background in statistics.\\n    * Python is generally considered easier to learn, with a more intuitive syntax.\\n\\n* **Community Support:**\\n    * Both R and Python have large and active communities, providing extensive documentation, tutorials, and support forums.\\n\\n**Recommendation:**\\n\\nIn today's world, **Python** is a more versatile and widely applicable choice for the following reasons:\\n\\n* **Versatility:** Python can be used for a wide range of tasks, including data science, machine learning, web development, and automation.\\n* **Growing Popularity:** Python has become increasingly popular in recent years, making it a valuable skill in various industries.\\n* **Ease of Learning:** Python has a relatively low learning curve, making it accessible to beginners.\\n* **Community Support:** Python has a large and active community, providing ample resources and support.\\n\\nHowever, if you are specifically interested in data science and statistics, **R** remains a powerful tool with a strong community and a wealth of specialized packages.\\n\\nUltimately, the best choice depends on your individual career goals and preferences. If you are unsure, consider learning the basics of both languages to determine which one aligns better with your interests and aspirations.\", response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "############################################################################\n",
    "# Chat models With ChatGoogleGenerativeAI\n",
    "############################################################################\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\", temperature=0.3, convert_system_message_to_human=True\n",
    ")\n",
    "\n",
    "gemini_llm(\n",
    "    [\n",
    "        SystemMessage(content=(\"You are a technical AI assistant\")),\n",
    "        HumanMessage(\n",
    "            content=\"Should i learn R programming or python in today's world?\"\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Schema & Message**\n",
    "Langchain’s Schema comprises three message types: HumanMessage, SystemMessage, and AIMessage. HumanMessage represents user input, SystemMessage provides system-generated responses, and AIMessage contains AI-generated outputs. These types facilitate user interactions, convey system information, and deliver AI responses. Schema ensures clarity and coherence in communication between users and AI models, streamlining development and improving user experience in AI applications.\n",
    "\n",
    "##### **Messages Prompt Template**\n",
    "In this case, we will be more detailed in the HumanMessage.\n",
    "\n",
    "We specify our requirement, and also the output format is JSON.\n",
    "Instruct AI to carry out sentiment analysis either positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n  \"sentiment\": \"Negative\"\\n}\\n```', response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are an AI assistant that you process the information and give sentiment of the information either as Positive, Negative or Neutral and you will give the output in the Json format\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chat_message = chat_template.format_messages(text=\"RCB has never won IPL\")\n",
    "\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\", temperature=0.3, convert_system_message_to_human=True\n",
    ")\n",
    "gemini_llm.invoke(chat_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we have used the built-in features of Langchain to structure and streamline the interaction between the user and AI. The output seems to be fine but can we make it interactive with prompts as we had done in the earlier section of prompt templates? Let’s try that in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Negative'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "# ------------------------ Formatting the output---------------------------------\n",
    "################################################################################################\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# define the parser object\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# create a chain\n",
    "chain = gemini_llm | parser\n",
    "\n",
    "sentiment = chain.invoke(chat_message)\n",
    "\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Prompt Feedback**\n",
    "AI-generated responses may sometimes contain errors or raise ethical concerns, highlighting the challenges of deploying AI technology responsibly. It’s essential to address these issues to maintain ethical standards and promote responsible AI usage. Gemini AI tackles this by providing prompt feedback, and helping users monitor and improve the quality of AI-generated content. This approach fosters transparency and accountability, contributing to a more trustworthy and reliable AI ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n",
       "  'probability': 'NEGLIGIBLE',\n",
       "  'blocked': False},\n",
       " {'category': 'HARM_CATEGORY_HATE_SPEECH',\n",
       "  'probability': 'NEGLIGIBLE',\n",
       "  'blocked': False},\n",
       " {'category': 'HARM_CATEGORY_HARASSMENT',\n",
       "  'probability': 'NEGLIGIBLE',\n",
       "  'blocked': False},\n",
       " {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',\n",
       "  'probability': 'NEGLIGIBLE',\n",
       "  'blocked': False}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#############################################################################\n",
    "# -------------------------Extracting the safety settings--------------------\n",
    "#############################################################################\n",
    "\n",
    "response = gemini_llm.invoke(chat_message)\n",
    "safety_ratings = response.response_metadata.get(\"safety_ratings\", [])\n",
    "safety_ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Memory**\n",
    "Memory in Langchain is the ability of AI models to retain and recall information from past interactions, enhancing the continuity and coherence of interactions. This feature improves user engagement and satisfaction by creating a more natural conversational experience. Memory also allows AI models to adapt to evolving user needs, making them more effective and efficient in serving users’ requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! How can I help you today?\n",
      "Human: I'm doing well! can you tell me what are business intelligence tools?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! How can I help you today?\n",
      "Human: I'm doing well! can you tell me what are business intelligence tools?\n",
      "AI: Business intelligence (BI) tools are software applications that help businesses collect, analyze, and visualize data to make better decisions. BI tools can be used to track key performance indicators (KPIs), identify trends, and forecast future performance. Some of the most popular BI tools include Microsoft Power BI, Tableau, and QlikView.\n",
      "Human: Who was the first president of the United States?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! How can I help you today?\n",
      "Human: I'm doing well! can you tell me what are business intelligence tools?\n",
      "AI: Business intelligence (BI) tools are software applications that help businesses collect, analyze, and visualize data to make better decisions. BI tools can be used to track key performance indicators (KPIs), identify trends, and forecast future performance. Some of the most popular BI tools include Microsoft Power BI, Tableau, and QlikView.\n",
      "Human: Who was the first president of the United States?\n",
      "AI: George Washington\n",
      "Human: Which web framework is the best in performance?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! How can I help you today?\n",
      "Human: I'm doing well! can you tell me what are business intelligence tools?\n",
      "AI: Business intelligence (BI) tools are software applications that help businesses collect, analyze, and visualize data to make better decisions. BI tools can be used to track key performance indicators (KPIs), identify trends, and forecast future performance. Some of the most popular BI tools include Microsoft Power BI, Tableau, and QlikView.\n",
      "Human: Who was the first president of the United States?\n",
      "AI: George Washington\n",
      "Human: Which web framework is the best in performance?\n",
      "AI: I do not have enough information to answer this question.\n",
      "Human: What was my previous question?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! How can I help you today?\n",
      "Human: I'm doing well! can you tell me what are business intelligence tools?\n",
      "AI: Business intelligence (BI) tools are software applications that help businesses collect, analyze, and visualize data to make better decisions. BI tools can be used to track key performance indicators (KPIs), identify trends, and forecast future performance. Some of the most popular BI tools include Microsoft Power BI, Tableau, and QlikView.\n",
      "Human: Who was the first president of the United States?\n",
      "AI: George Washington\n",
      "Human: Which web framework is the best in performance?\n",
      "AI: I do not have enough information to answer this question.\n",
      "Human: What was my previous question?\n",
      "AI: Which web framework is the best in performance?\n",
      "Human: Thank you\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You're welcome! Is there anything else I can help you with today?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=gemini_llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")\n",
    "conversation.predict(\n",
    "    input=\"I'm doing well! can you tell me what are business intelligence tools?\"\n",
    ")\n",
    "conversation.predict(input=\"Who was the first president of the United States?\")\n",
    "conversation.predict(input=\"Which web framework is the best in performance?\")\n",
    "conversation.predict(input=\"What was my previous question?\")\n",
    "conversation.predict(input=\"Thank you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the chat history is maintained in the memory and AI can refer to the history and respond accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Agents**\n",
    "Large Language Models (LLMs) have impressive capabilities but lack basic functions like logic and calculation. Agents, equipped with specialized toolkits, perform specific tasks. For example, Python Agent uses PythonREPLTool to execute commands. The LLM instructs the agent on which code to run, and flexible chains of calls are determined by user input or an Agent can search for information on Google, fetch the information, and initiate the next sequence. This approach ensures efficient and adaptable AI interactions tailored to user needs.\n",
    "\n",
    "Let’s create an Agent with a basic example.\n",
    "\n",
    "We will create a REPL tool that uses a Python shell to execute Python commands.\n",
    "Build an Agent that uses this tool to process the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mAction: python_repl\n",
      "Action Input: principal = 10000\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: python_repl\n",
      "Action Input: interest_rate = 5.25 / 100\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: python_repl\n",
      "Action Input: years = 3\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: python_repl\n",
      "Action Input: interest = principal * interest_rate * years\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: python_repl\n",
      "Action Input: print(interest)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1575.0\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 1575\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1575'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent_name=\"gemini_agent\", llm=gemini_llm, verbose=True, tools=[repl_tool]\n",
    ")\n",
    "\n",
    "agent.run(\"How much interest wll I get for 10000 at 5.25 percent for 3 years?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Conclusion**\n",
    "Google’s Gemini AI and Langchain are a powerful combination in AI development. Gemini-pro’s latest API updates provide developers with enhanced features for building sophisticated AI applications. Langchain’s seamless integration with Gemini enhances its potential, offering a robust framework for harnessing Gemini AI’s full power. As demand for intelligent AI solutions grows, Langchain stands at the forefront, providing developers with the tools to succeed in this rapidly evolving landscape. The possibilities are limitless with these two AI tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
